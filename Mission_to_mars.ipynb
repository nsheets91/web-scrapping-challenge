{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import requests\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Get LATEST driver version for 92.0.4515\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/92.0.4515.107/chromedriver_mac64.zip\n",
      "Driver has been saved in cache [/Users/nicksheets/.wdm/drivers/chromedriver/mac64/92.0.4515.107]\n"
     ]
    }
   ],
   "source": [
    "# creating driver and browser\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of news page to be scraped and visit it with browser\n",
    "news_url = 'https://redplanetscience.com/'\n",
    "browser.visit(news_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"The agency's Mars 2020 mission is on its way. It will land at Jezero Crater in about seven months, on Feb. 18, 2021. \""
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating HTML object\n",
    "news_html = browser.html\n",
    "\n",
    "# initiating soup object for news scrape\n",
    "news_soup = BeautifulSoup(news_html, 'html.parser')\n",
    "\n",
    "# searching html for latest news\n",
    "latest_news = news_soup.find('section', class_='image_and_description_container')\n",
    "title = latest_news.find(\"div\", class_='content_title').text\n",
    "paragraph = latest_news.find(\"div\", class_='article_teaser_body').text\n",
    "paragraph\n",
    "# getting title and paragraph for latest article\n",
    "# title = latest_news.find('div', class_='content_title').text\n",
    "# paragraph = latest_news.find('div', class_='article_teaser_body').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of image page to be scraped and visit it with browser\n",
    "mars_url = 'https://spaceimages-mars.com'\n",
    "browser.visit(mars_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'https://spaceimages-mars.com/image/featured/mars2.jpg'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HTML object\n",
    "html = browser.html\n",
    "\n",
    "# instatiating beautiful soup object and parsing using lxml\n",
    "image_soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# accessing the image url\n",
    "space_image_url = image_soup.find('a', class_='showimg fancybox-thumbs')['href']\n",
    "\n",
    "# attaching the image url to sliced page url\n",
    "image_url = f\"https://spaceimages-mars.com/{space_image_url}\"\n",
    "image_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of facts page to be scraped and visit it with browser\n",
    "facts_url = 'https://galaxyfacts-mars.com'\n",
    "\n",
    "# parsing html using pandas\n",
    "facts_tables = pd.read_html(facts_url)\n",
    "facts_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning pandas parse into a dataframe\n",
    "facts_df = facts_tables[0]\n",
    "\n",
    "# cleaning up the table for printing\n",
    "facts_df = facts_df.rename(columns={0: \" \", 1: \"Mars\"})\n",
    "facts_df.set_index(\" \", inplace=True)\n",
    "\n",
    "# converting df to html with pandas\n",
    "facts_html = facts_df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(facts_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of image page to be scraped and visit it with browser\n",
    "hemi_image_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(hemi_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of image page to be scraped and visit it with browser\n",
    "hemi_image_url = 'https://marshemispheres.com/'\n",
    "browser.visit(hemi_image_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML object\n",
    "html = browser.html\n",
    "\n",
    "# instatiating beautiful soup object and parsing with html\n",
    "hemi_image_soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# accessing the names of the links to click and storing in a list to iterate over\n",
    "hemi_names = hemi_image_soup.find_all('div', class_='description')\n",
    "hemi_name_list = []\n",
    "for name in hemi_names:\n",
    "    hemi_name_list.append(name.a.h3.text)\n",
    "\n",
    "# creating list for dicts for return\n",
    "hemi_list = []\n",
    "\n",
    "# looping over the pages, scraping, creating dict, and adding to list\n",
    "for name in hemi_name_list:\n",
    "        try:\n",
    "            # clicking into the page to get the image url\n",
    "            browser.links.find_by_partial_text(name).click()\n",
    "            \n",
    "            # getting image_url\n",
    "            html = browser.html\n",
    "            image_url_soup = BeautifulSoup(html, 'html.parser')\n",
    "            dl_image_url = image_url_soup.find_all('dd')[1].a['href']\n",
    "            image_url = f\"{dl_image_url}/full.jpg\"\n",
    "            \n",
    "            # getting title from name\n",
    "            title = name[0:-9]\n",
    "            \n",
    "            # creating dict and adding to list\n",
    "            entry = {'title': title, 'image_url': image_url, 'dl_image_url': dl_image_url}\n",
    "            hemi_list.append(entry)\n",
    "            \n",
    "            # redirecting back to the main page to continue loop\n",
    "            browser.visit(hemi_image_url)\n",
    "            \n",
    "            print(\"Scrape successful!\")\n",
    "            \n",
    "        except:\n",
    "            print(\"Scrape unsuccessful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}